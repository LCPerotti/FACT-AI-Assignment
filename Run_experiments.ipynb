{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot recreation\n",
    "This notebook will run the code used to recreate the plots from the paper. There will be an option to either run the code to generate the required CSVs, or to only run the plotting code. The code to run the CSVs is expected to take multiple hours all together given an available GPU, so is not recommended to be ran on lower-end hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wm3T_hfi8Vs"
   },
   "outputs": [],
   "source": [
    "## Packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose to run full experiments\n",
    "\n",
    "To run the full experiments from the paper, set the variable CREATE_CSVS to True. Do be warned that this will likely take a lot of time, and require a lot of VRAM for the softmax experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true to run the full scripts to generate the CSVs. This might take multiple hours, and is not recommended to be run without a GPU.\n",
    "CREATE_CSVS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs all experiments if CREATE_CSVS is set.\n",
    "\n",
    "Since the logit attribution experiment creates a lot of print statements, to reduce clutter in the notebook, the output will not be shown. The required CSV will still be made. If the outputs and tqdm time estimate is wanted, set SUPRESS_STDOUT_ATTRIBUTION to False.\n",
    "\n",
    "Other experiments will still show their outputs and tqdm time estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit-attribution creates a ton of print statements to stdout. To remove clutter, stdout will be discarded when running these.\n",
    "# If stdout is wanted, set to False.\n",
    "\n",
    "if os.getcwd().split(os.sep)[-1] != \"original_paper_code\":\n",
    "  os.chdir(\"original_paper_code\")\n",
    "SUPRESS_STDOUT_ATTRIBUTION = True\n",
    "if CREATE_CSVS:\n",
    "  os.chdir(\"Script\")\n",
    "  \n",
    "  if SUPRESS_STDOUT_ATTRIBUTION:\n",
    "    print(\"Running logit attribution (intermediate output is discarded to reduce clutter)\")\n",
    "    original_stdout = sys.stdout\n",
    "    discard = open(os.devnull, 'w')\n",
    "    sys.stdout = discard\n",
    "\n",
    "  # Run logit attribution (seperate to discard stdout)\n",
    "  !python run_all.py --logit-attribution\n",
    "\n",
    "  # RUn softmax logit attribution\n",
    "  !python run_all.py --logit-attribution --normalize-logit softmax --flag softmax --batch 5\n",
    "\n",
    "  if SUPRESS_STDOUT_ATTRIBUTION:\n",
    "    sys.stdout = original_stdout\n",
    "    print(\"Logit attribution finished.\")\n",
    "    \n",
    "  \n",
    "  # Run rest of original experiments\n",
    "  !python run_all.py --logit-lens --pattern --no-plot\n",
    "\n",
    "  # Run rest of softmax logit experiments\n",
    "  !python run_all.py --logit-lens --no-plot --normalize-logit softmax --flag softmax\n",
    "\n",
    "\n",
    "  os.chdir(\"..\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Gt82vABi5dG"
   },
   "outputs": [],
   "source": [
    "## Loading in precomputed data sets\n",
    "regular_dir = \"results/copyVSfact\"\n",
    "softmax_dir = \"results/copyVSfactsoftmax\"\n",
    "logit_attribution_dir = \"/logit_attribution/gpt2_full/logit_attribution_data.csv\"\n",
    "logit_lens_dir = \"/logit_lens/gpt2_full/logit_lens_data.csv\"\n",
    "head_pattern_dir = \"/head_pattern/gpt2_full/head_pattern_data.csv\"\n",
    "\n",
    "logit_attribution_data = pd.read_csv(regular_dir + logit_attribution_dir)\n",
    "logit_attribution_data_softmax = pd.read_csv(softmax_dir + logit_attribution_dir)\n",
    "logit_lens_data = pd.read_csv(regular_dir + logit_lens_dir)\n",
    "logit_lens_data_softmax = pd.read_csv(softmax_dir + logit_lens_dir)\n",
    "head_pattern_data = pd.read_csv(regular_dir + head_pattern_dir)\n",
    "\n",
    "fig_width, fig_height = 7,5\n",
    "dpi = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaNpJpyAtXtZ"
   },
   "outputs": [],
   "source": [
    "## Functions for reproducing plots\n",
    "\n",
    "def plot_intermediate_logits(data): # takes logit_lens_data\n",
    "\n",
    "    # Extract the data for plotting\n",
    "    data = data[data['position'] == 13]\n",
    "    layers = data['layer']\n",
    "    mem_values = data['mem']\n",
    "    cp_values = data['cp']\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "\n",
    "    # Plot 'mem' values in blue with thicker lines\n",
    "    plt.plot(layers, mem_values, label='Factual Token', marker='o', color='blue', linewidth=3)\n",
    "\n",
    "    # Plot 'cp' values in red with thicker lines\n",
    "    plt.plot(layers, cp_values, label='Counterfactual token', marker='o', color='red', linewidth=3)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Layer', fontsize=18)\n",
    "    plt.ylabel('Logits in the Last Position', fontsize=18)\n",
    "\n",
    "    # Set y-axis tick marks\n",
    "    plt.yticks([0, 5, 10, 15])\n",
    "\n",
    "    # Place the legend\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Adjust layout to add padding\n",
    "    plt.tight_layout(pad=1.0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_block_attribution_logits(data): # takes logit_attribution_data\n",
    "\n",
    "    # Filter data where position == 13\n",
    "    data = data[data['position'] == 13]\n",
    "\n",
    "    # Extract ATT block data\n",
    "    attn_out_data = data[data['label'].str.contains('attn_out')]\n",
    "    att_diff = attn_out_data['cp_mean'] - attn_out_data['mem_mean']\n",
    "\n",
    "    # Extract MLP block data\n",
    "    mlp_out_data = data[data['label'].str.contains('mlp_out')]\n",
    "    mlp_diff = mlp_out_data['cp_mean'] - mlp_out_data['mem_mean']\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), dpi=600)\n",
    "\n",
    "    # Define layers for plotting\n",
    "    layers = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "    # Plot ATT block data\n",
    "    axes[0].bar(layers, att_diff, color='orange', zorder=3)\n",
    "    axes[0].set_xlabel('Layers')\n",
    "    axes[0].set_ylabel(r'$\\Delta_\\text{cofa}$')\n",
    "    axes[0].set_title('Attention Block')\n",
    "    axes[0].tick_params(axis='x')\n",
    "\n",
    "    # Plot MLP block data\n",
    "    axes[1].bar(layers, mlp_diff, color='purple', zorder=3)\n",
    "    axes[1].set_xlabel('Layers')\n",
    "    #axes[1].set_ylabel(r'$\\Delta t_\\text{cofa}$')\n",
    "    axes[1].set_title('MLP Block')\n",
    "    axes[1].tick_params(axis='x')\n",
    "\n",
    "    # Set x-axis to show layer values and scale y-axis\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(range(len(layers)))\n",
    "        ax.set_xticklabels(layers)\n",
    "        ax.set_ylim([-1, 1.5])\n",
    "        ax.set_yticks([i * 0.5 for i in range(-2, 4)])  # y-axis ticks separated by 0.5\n",
    "        ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "        ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))\n",
    "        ax.grid(which='both', axis='y', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_head_attribution_logits(data): # takes logit_attribution_data\n",
    "\n",
    "    # Filter data where position == 13\n",
    "    data = data[data['position'] == 13]\n",
    "\n",
    "    # Filter labels that start with \"L\"\n",
    "    data = data[data['label'].str.startswith('L', na=False)]\n",
    "\n",
    "    # Extract layer and head information\n",
    "    data['layer'] = data['label'].apply(lambda x: int(x.split('H')[0][1:]))\n",
    "    data['head'] = data['label'].apply(lambda x: int(x.split('H')[1]))\n",
    "\n",
    "    # Compute difference\n",
    "    data['diff'] = data['cp_mean'] - data['mem_mean']\n",
    "\n",
    "    # Create a pivot table and transpose it\n",
    "    pivot_table = data.pivot_table(values='diff', index='layer', columns='head').T\n",
    "\n",
    "    # Plot the heatmap using seaborn\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "    ax = sns.heatmap(\n",
    "        pivot_table,\n",
    "        cmap='seismic',\n",
    "        center=0,\n",
    "        cbar=True,  # Enable colorbar\n",
    "        cbar_kws={'orientation': 'vertical', 'shrink': 0.6, 'aspect': 10},\n",
    "        linewidths=0.5,\n",
    "        linecolor='grey',\n",
    "        square=True,\n",
    "        vmin=-1,\n",
    "        vmax=1)\n",
    "\n",
    "    # Reverse the y-axis\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Adjust colorbar position and size\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.set_title(r'$\\Delta_\\text{cofa}$', fontsize=16, pad=10)\n",
    "    cbar.ax.set_anchor((0.5, 0.5))  # Center the colorbar vertically\n",
    "\n",
    "    # Customize the headers\n",
    "    plt.xlabel('Layer', fontsize=16)\n",
    "    plt.ylabel('Head', fontsize=16)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_per_token(data): #takes head_pattern_data\n",
    "\n",
    "    # Define the specific (layer, head) pairs we are analyzing\n",
    "    LayerHead = [(9,6), (9,9), (10,0), (10,7), (10,10), (11,10)]\n",
    "\n",
    "    # Assuming head_pattern_data is already loaded\n",
    "    data = head_pattern_data\n",
    "\n",
    "    # Initialize an empty DataFrame to store the combined data\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through each (layer, head) pair and concatenate the filtered data\n",
    "    for layer, head in LayerHead:\n",
    "        # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "        filtered_data = data[(data['layer'] == layer) &\n",
    "                            (data['head'] == head) &\n",
    "                            (data['source_position'] == 13)].copy()\n",
    "\n",
    "        # Assign 'layer_head' column safely\n",
    "        filtered_data.loc[:, 'layer_head'] = f'Layer {layer} | Head {head}'\n",
    "\n",
    "        # Change the sign for specific (layer, head) pairs\n",
    "        if (layer, head) in [(10,7), (11,10)]:\n",
    "            filtered_data.loc[:, 'value'] = -filtered_data['value']\n",
    "\n",
    "        # Append the modified data\n",
    "        combined_data = pd.concat([combined_data, filtered_data], ignore_index=True)\n",
    "\n",
    "    # Set the order of the 'layer_head' category in reverse order\n",
    "    combined_data['layer_head'] = pd.Categorical(\n",
    "        combined_data['layer_head'],\n",
    "        categories=[f'Layer {layer} | Head {head}' for layer, head in reversed(LayerHead)],\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Pivot the table to get the destination positions as columns\n",
    "    pivot_table_combined = combined_data.pivot(index='layer_head', columns='dest_position', values='value')\n",
    "\n",
    "    # Set a fixed color range for the heatmap to avoid extreme dark colors\n",
    "    vmin, vmax = -1, 1\n",
    "\n",
    "    # Create the main heatmap with fixed color limits\n",
    "    plt.figure(figsize=(2*fig_width, fig_height), dpi=dpi)\n",
    "    ax = sns.heatmap(\n",
    "        pivot_table_combined,\n",
    "        cmap='seismic',  # Use diverging colormap for heatmap\n",
    "        center=0,  # Center the colormap at 0\n",
    "        cbar=False,  # Disable the default color bar\n",
    "        linewidths=0.1,\n",
    "        linecolor='grey',\n",
    "        square=True,\n",
    "        vmin=vmin,  # Set minimum color value\n",
    "        vmax=vmax    # Set maximum color value\n",
    "    )\n",
    "\n",
    "    # Set the x-axis label\n",
    "    plt.xlabel('Token Position', fontsize=12)\n",
    "\n",
    "    # Create a divider for the axes\n",
    "    divider = make_axes_locatable(ax)\n",
    "\n",
    "    # Add the first color bar (negative values - blue)\n",
    "    cax_neg = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "    # Set the label at the top with a multi-line format\n",
    "    cax_neg.set_title(\"Factual\\nAttention\", loc='left', fontsize=10)\n",
    "\n",
    "    # Add the second color bar (positive values - red)\n",
    "    cax_pos = divider.append_axes(\"right\", size=\"5%\", pad=0.8)\n",
    "\n",
    "    # Set the label at the top with a multi-line format\n",
    "    cax_pos.set_title(\"Counterfactual\\nAttention\", loc='left', fontsize=10)\n",
    "\n",
    "    # Remove y-axis label explicitly\n",
    "    ax.set_ylabel('')\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)  # Adjusts space to ensure full removal\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuYOYtbQcgH7"
   },
   "outputs": [],
   "source": [
    "## Functions for new plots\n",
    "\n",
    "def plot_intermediate_logits_softmax(data): # takes logit_lens_data\n",
    "\n",
    "    # Extract the data for plotting\n",
    "    data = data[data['position'] == 13]\n",
    "    layers = data['layer']\n",
    "    mem_values = data['mem']\n",
    "    cp_values = data['cp']\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "\n",
    "    # Plot 'mem' values in blue with thicker lines\n",
    "    plt.plot(layers, mem_values, label='Factual Token', marker='o', color='blue', linewidth=3)\n",
    "\n",
    "    # Plot 'cp' values in red with thicker lines\n",
    "    plt.plot(layers, cp_values, label='Counterfactual token', marker='o', color='red', linewidth=3)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Layer', fontsize=18)\n",
    "    plt.ylabel('Probabilities in the Last Position', fontsize=18)\n",
    "\n",
    "    # Set y-axis to logarithmic scale\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Place the legend underneath the plot area but inside the figure\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Adjust layout to add padding\n",
    "    plt.tight_layout(pad=1.0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_block_attribution_logits_softmax(data): # takes logit_attribution_data\n",
    "\n",
    "    # Filter data where position == 13\n",
    "    data = data[data['position'] == 13]\n",
    "\n",
    "    # Extract ATT block data\n",
    "    attn_out_data = data[data['label'].str.contains('attn_out')]\n",
    "    att_diff = attn_out_data['cp_mean'] - attn_out_data['mem_mean']\n",
    "\n",
    "    # Extract MLP block data\n",
    "    mlp_out_data = data[data['label'].str.contains('mlp_out')]\n",
    "    mlp_diff = mlp_out_data['cp_mean'] - mlp_out_data['mem_mean']\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(2*fig_width, fig_height), dpi=dpi)\n",
    "\n",
    "    # Define layers for plotting\n",
    "    layers = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "    # Plot ATT block data\n",
    "    axes[0].bar(layers, att_diff, color='orange', zorder=3)\n",
    "    axes[0].set_xlabel('Layers')\n",
    "    axes[0].set_ylabel(r'$\\widetilde\\Delta_\\text{cofa}$')\n",
    "    axes[0].set_title('Attention Block')\n",
    "    axes[0].tick_params(axis='x')\n",
    "\n",
    "    # Plot MLP block data\n",
    "    axes[1].bar(layers, mlp_diff, color='purple', zorder=3)\n",
    "    axes[1].set_xlabel('Layers')\n",
    "    axes[1].set_title('MLP Block')\n",
    "    axes[1].tick_params(axis='x')\n",
    "\n",
    "    # Set x-axis to show layer values and scale y-axis\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(range(len(layers)))\n",
    "        ax.set_xticklabels(layers)\n",
    "        ax.set_ylim(-1e-4, 6e-4)\n",
    "        ax.set_yticks(np.arange(-1e-4, 6.5e-4, 1e-4))\n",
    "        ax.yaxis.set_major_locator(plt.MultipleLocator(1e-4))\n",
    "        ax.yaxis.set_minor_locator(plt.MultipleLocator(5e-5))\n",
    "        ax.grid(which='both', axis='y', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_block_attribution_logits_softmax_merged(data):  # Takes logit_attribution_data_softmax\n",
    "\n",
    "    # Filter data where position == 13\n",
    "    data = data[data['position'] == 13].copy()\n",
    "\n",
    "    # Filter data for attn_out and mlp_out\n",
    "    attn_out_data = data[data['label'].str.contains('attn_out', na=False)].copy()\n",
    "    mlp_out_data = data[data['label'].str.contains('mlp_out', na=False)].copy()\n",
    "\n",
    "    # Extract layer numbers from labels safely\n",
    "    attn_out_data.loc[:, 'layer'] = pd.to_numeric(attn_out_data['label'].str.extract(r'(\\d+)')[0], errors='coerce')\n",
    "    mlp_out_data.loc[:, 'layer'] = pd.to_numeric(mlp_out_data['label'].str.extract(r'(\\d+)')[0], errors='coerce')\n",
    "\n",
    "    # Drop NaN values before converting to int\n",
    "    attn_out_data = attn_out_data.dropna(subset=['layer'])\n",
    "    mlp_out_data = mlp_out_data.dropna(subset=['layer'])\n",
    "\n",
    "    # Convert to integer after handling NaNs\n",
    "    attn_out_data.loc[:, 'layer'] = attn_out_data['layer'].astype(int)\n",
    "    mlp_out_data.loc[:, 'layer'] = mlp_out_data['layer'].astype(int)\n",
    "\n",
    "    # Sort data by layer\n",
    "    attn_out_data = attn_out_data.sort_values('layer')\n",
    "    mlp_out_data = mlp_out_data.sort_values('layer')\n",
    "\n",
    "    # Create a combined dataframe\n",
    "    combined_data = pd.merge(attn_out_data, mlp_out_data, on='layer', suffixes=('_attn', '_mlp'))\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "\n",
    "    bar_width = 0.45\n",
    "    index = combined_data['layer']\n",
    "\n",
    "    # Plot attn_out and mlp_out data side by side\n",
    "    bar1 = ax.bar(index - bar_width/2, combined_data['cp_mean_attn'] - combined_data['mem_mean_attn'], bar_width, label='Attention Block', color='orange', zorder=3)\n",
    "    bar2 = ax.bar(index + bar_width/2, combined_data['cp_mean_mlp'] - combined_data['mem_mean_mlp'], bar_width, label='MLP Block', color='purple', zorder=3)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Layers')\n",
    "    ax.set_ylabel(r'$\\widetilde\\Delta_\\text{cofa}$')\n",
    "    ax.set_title('Attention Block vs MLP Block')\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(index)\n",
    "    ax.set_ylim([-0.00015, .0006])\n",
    "    ax.grid(which='both', axis='y', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_head_attribution_logits_softmax(data): # takes logit_attribution_data\n",
    "\n",
    "    # Filter data where position == 13\n",
    "    data = data[data['position'] == 13]\n",
    "\n",
    "    # Filter labels that start with \"L\"\n",
    "    data = data[data['label'].str.startswith('L', na=False)]\n",
    "\n",
    "    # Extract layer and head information\n",
    "    data['layer'] = data['label'].apply(lambda x: int(x.split('H')[0][1:]))\n",
    "    data['head'] = data['label'].apply(lambda x: int(x.split('H')[1]))\n",
    "\n",
    "    # Compute difference\n",
    "    data['diff'] = data['cp_mean'] - data['mem_mean']\n",
    "\n",
    "    # Create a pivot table and transpose it\n",
    "    pivot_table = data.pivot_table(values='diff', index='layer', columns='head').T\n",
    "\n",
    "    # Determine the limits for the color scale\n",
    "    vmax = max(abs(pivot_table.min().min()), abs(pivot_table.max().max()))\n",
    "\n",
    "    # Plot the heatmap using seaborn\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "    ax = sns.heatmap(\n",
    "        pivot_table,\n",
    "        cmap='seismic',\n",
    "        center=0,\n",
    "        cbar = True,\n",
    "        cbar_kws={'orientation': 'vertical', 'shrink': 0.6, 'aspect': 10},\n",
    "        linewidths=0.5,\n",
    "        linecolor='grey',\n",
    "        square=True,\n",
    "        vmin=-vmax,\n",
    "        vmax=vmax)\n",
    "\n",
    "    # Reverse the y-axis\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Adjust colorbar position and size\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.set_title(r'$\\widetilde\\Delta_\\text{cofa}$', fontsize=16, pad=10)\n",
    "    cbar.ax.set_anchor((0.5, 0.5))  # Center the colorbar vertically\n",
    "\n",
    "    # Customize the headers\n",
    "    plt.xlabel('Layer', fontsize=16)\n",
    "    plt.ylabel('Head', fontsize=16)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl3MBogbizU7"
   },
   "source": [
    "# We start with the plots of the original paper\n",
    "\n",
    "The first set of plots are from the replication study, where both the code and plotting strategy are replicated from Ortu et al. (2024). We mention the original Section and Figure ID, then show our own corresponding reproduction resutls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdyJ9CgTtbng"
   },
   "source": [
    "## Section 6.1 Figure 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "xhxcq6s49dBO",
    "outputId": "d4742ab6-30e6-4e57-af08-a94acab15e3c"
   },
   "outputs": [],
   "source": [
    "plot_intermediate_logits(logit_lens_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xekE6WqztpiO"
   },
   "source": [
    "## Section 6.2 Figure 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "BqD2Ja739eYN",
    "outputId": "44ac8fa4-6c3a-4611-d1f5-a4004d1220e9"
   },
   "outputs": [],
   "source": [
    "plot_block_attribution_logits(logit_attribution_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YFJ_4PBtxh4"
   },
   "source": [
    "## Section 6.3 Figure 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "DOwAHfsD9d-x",
    "outputId": "d1e1dc79-e829-446d-e772-25cc77b22193"
   },
   "outputs": [],
   "source": [
    "plot_head_attribution_logits(logit_attribution_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlTkvLey9vGv"
   },
   "source": [
    "## Section 6.3 Figure 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "kdGW_3Ad91is",
    "outputId": "a1e2ea18-c3ad-4d0f-b2f0-f734d7f34fd9"
   },
   "outputs": [],
   "source": [
    "plot_attention_per_token(head_pattern_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCJWDODCH213"
   },
   "source": [
    "# Further experiments\n",
    "\n",
    "Now we give the plots of the extended experiments, starting with softmax experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "Cf1VGKTxIUwC",
    "outputId": "7d7460ae-6e1b-4d6c-a07c-1754eb6923c8"
   },
   "outputs": [],
   "source": [
    "plot_intermediate_logits_softmax(logit_lens_data_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "TubicJEtIr55",
    "outputId": "90af33f7-4f79-4d8d-b892-5b0c163b8f52"
   },
   "outputs": [],
   "source": [
    "plot_block_attribution_logits_softmax(logit_attribution_data_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "b0C_nFF7Vo9u",
    "outputId": "d5adf9ff-05fb-41b8-ac89-5cab9e7124be"
   },
   "outputs": [],
   "source": [
    "plot_block_attribution_logits_softmax_merged(logit_attribution_data_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "4jmNiuGgRJpo",
    "outputId": "bdbf02ca-d124-4312-8c49-eb8012f1df1d"
   },
   "outputs": [],
   "source": [
    "plot_head_attribution_logits_softmax(logit_attribution_data_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from Src.model import ModelFactory\n",
    "from Src.dataset import BaseDataset\n",
    "from Src.experiment import Ablator\n",
    "\n",
    "sys.path.append('./src')\n",
    "sys.path.append('.gitignore/data')\n",
    "\n",
    "if CREATE_CSVS:\n",
    "  model = ModelFactory.create(\"gpt2\")\n",
    "  dataset = BaseDataset(path = \"data/full_data_sampled_gpt2.json\",\n",
    "                      model = model,\n",
    "                      experiment=\"copyVSfact\",\n",
    "                      no_subject=True)\n",
    "  ablator = Ablator(model=model, dataset=dataset, experiment=\"copyVSfact\", batch_size=20)\n",
    "\n",
    "COFA_HEADS = [(7, 10), (9, 9), (9, 6), (10, 0)]\n",
    "FA_HEADS = [(10, 7), (11, 10), (11, 3)]\n",
    "\n",
    "FA_ALPHAS = [1, 2, 5, 10, 100]\n",
    "COFA_ALPHAS = [1, 0.5, 0.2, 0.1, 0]\n",
    "\n",
    "\n",
    "FA_ALPHAS = [1]\n",
    "COFA_ALPHAS = [1]\n",
    "\n",
    "# computed results\n",
    "BEST_BOOST = [((10, 7), (11, 10))]\n",
    "BEST_SURPRESS = [((7, 10), (9, 9), (9, 6), (10, 0)), ((7, 10), (9, 9), (10, 0)), ((9, 9), (9, 6), (10, 0))]\n",
    "\n",
    "def one_run_df(ablator, alpha, name_of_experiment):\n",
    "    cur_df = ablator.run()\n",
    "    cur_df['alpha'] = alpha\n",
    "    cur_df['experiment'] = name_of_experiment\n",
    "    return cur_df\n",
    "\n",
    "def subset_attn_modif(ablator, heads, name_of_experiment, alpha=0):\n",
    "    result = []\n",
    "    for subset_size in range(1, len(heads)+1):\n",
    "        for heads_subset in itertools.combinations(heads, subset_size):\n",
    "            ablator.set_heads(heads=list(heads_subset), value=alpha, position='attribute')\n",
    "            cur_df = one_run_df(ablator, alpha, name_of_experiment)\n",
    "            cur_df['heads'] = str(heads_subset)\n",
    "            result.append(cur_df)\n",
    "    return pd.concat(result)\n",
    "\n",
    "\n",
    "def gridsearch_attn_modif(ablator, heads, alphas, name_of_experiment):\n",
    "    result = []\n",
    "    for alpha in alphas:\n",
    "        ablator.set_heads(heads=heads, value=alpha, position='attribute')\n",
    "        cur_df = one_run_df(ablator, alpha, name_of_experiment)\n",
    "        cur_df['heads'] = str(heads)\n",
    "        result.append(cur_df)\n",
    "    return pd.concat(result)\n",
    "\n",
    "\n",
    "def combination_attn_modif(ablator, heads_boost, heads_surpress, name_of_experiment, alpha_boost=5, alpha_surpress=0):\n",
    "    combined_result = []\n",
    "    for boost in heads_boost:\n",
    "        for surpress in heads_surpress:\n",
    "            ablator.set_heads(heads=list(boost), value=alpha_boost, position='attribute')\n",
    "            ablator.set_heads(heads=list(surpress), value=alpha_surpress, position='attribute', reset=False)\n",
    "            cur_df = one_run_df(ablator, alpha_boost, name_of_experiment)\n",
    "            cur_df['heads_boost'] = str(boost)\n",
    "            cur_df['heads_surpress'] = str(surpress)\n",
    "            cur_df['beta'] = alpha_surpress\n",
    "            combined_result.append(cur_df)\n",
    "\n",
    "    return pd.concat(combined_result)\n",
    "\n",
    "\n",
    "\n",
    "def run_attn_modif():\n",
    "    fa_results_grid = gridsearch_attn_modif(ablator, FA_HEADS, FA_ALPHAS, \"fa_gridsearch\")\n",
    "    cofa_results_grid = gridsearch_attn_modif(ablator, COFA_HEADS, COFA_ALPHAS, \"cofa_gridsearch\")\n",
    "\n",
    "    results_grid = pd.concat([fa_results_grid, cofa_results_grid])\n",
    "    results_grid.to_csv(\"results_grid.csv\", index=False)\n",
    "    print('Grid search done and saved')\n",
    "\n",
    "    fa_results_subset = subset_attn_modif(ablator, FA_HEADS, \"fa_subset_boost\", alpha=5)\n",
    "    cofa_results_subset = subset_attn_modif(ablator, COFA_HEADS, \"cofa_subset_ablate\")\n",
    "\n",
    "    results_subset = pd.concat([fa_results_subset, cofa_results_subset])\n",
    "    results_subset.to_csv(\"results_subset.csv\", index=False)\n",
    "    print('Subset search done and saved')\n",
    "\n",
    "    combined_results = combination_attn_modif(ablator, BEST_BOOST, BEST_SURPRESS, \"combined\", alpha_boost=5, alpha_surpress=0)\n",
    "    combined_results.to_csv(\"results_combined.csv\", index=False)\n",
    "    print('Combined search done and saved')\n",
    "\n",
    "    all_results = pd.concat([results_grid, results_subset, combined_results])\n",
    "    all_results.to_csv(\"all_results.csv\", index=False)\n",
    "    print('All done and saved')\n",
    "\n",
    "\n",
    "def single_boost(ablator, num, alpha=5, pos=\"attribute\"):\n",
    "    results = []\n",
    "    tried_heads = []\n",
    "    while len(tried_heads) < num:\n",
    "        heads = np.random.randint(0, 12), np.random.randint(0, 12)\n",
    "        if heads not in tried_heads:\n",
    "            tried_heads.append(heads)\n",
    "            ablator.set_heads([heads], position=pos, value=alpha, reset=True)\n",
    "            df = ablator.run()\n",
    "            df['heads'] = str(heads)\n",
    "            df['experiment'] = 'single_boost'\n",
    "            df['pos'] = pos\n",
    "            results.append(df)\n",
    "\n",
    "    results = pd.concat(results)\n",
    "    results.to_csv(f\"single_boost_{pos}.csv\", index=False)\n",
    "\n",
    "def double_boost(ablator, num, alpha=5, pos=\"attribute\"):\n",
    "    results2 = []\n",
    "    tried_tuples = []\n",
    "\n",
    "    while len(tried_tuples) < num:\n",
    "        head1, head2 = sorted(((np.random.randint(0, 12), np.random.randint(0, 12)) for _ in range(2)))\n",
    "        print(head1, head2)\n",
    "        if (head1, head2) not in tried_tuples:\n",
    "            tried_tuples.append((head1, head2))\n",
    "            ablator.set_heads([head1, head2], position=pos, value=alpha, reset=True)\n",
    "            df = ablator.run()\n",
    "            df['heads'] = f'{head1}, {head2}'\n",
    "            df['experiment'] = 'double_boost'\n",
    "            df['layer_diff'] = abs(head1[0] - head2[0])\n",
    "            df['pos'] = pos\n",
    "            results2.append(df)\n",
    "\n",
    "    results2 = pd.concat(results2)\n",
    "    results2.to_csv(f\"double_boost_{pos}.csv\", index=False)\n",
    "\n",
    "def random_modif():\n",
    "    np.random.seed(347)\n",
    "\n",
    "    single_boost(ablator, 50, 5, \"all\")\n",
    "    print('Single boost for all done and saved')\n",
    "    double_boost(ablator, 50, 5, \"all\")\n",
    "    print('Double boost for all done and saved')\n",
    "\n",
    "    single_boost(ablator, 50, 5, \"attribute\")\n",
    "    print('Single boost for attribute done and saved')\n",
    "    double_boost(ablator, 50, 5, \"attribute\")\n",
    "    print('Double boost for attribute done and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_CSVS:\n",
    "  run_attn_modif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_CSVS:\n",
    "  random_modif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_random_recall_by_layer(pos):\n",
    "    single_boost = pd.read_csv(f\"results/copyVSfact/random_attention_modif/single_boost_{pos}.csv\")\n",
    "    single_boost['heads'] = single_boost['heads'].apply(lambda x: eval(x))\n",
    "    single_boost['fact_recall'] = single_boost['mem_win'] / 100\n",
    "    by_layers = [single_boost[single_boost['heads'].apply(lambda x: x[0] == i)].describe()['fact_recall'] for i in range(12)]\n",
    "\n",
    "    by_layers = pd.concat(by_layers, axis=1).T\n",
    "    by_layers['layer'] = range(12)\n",
    "    by_layers.set_index('layer', inplace=True)\n",
    "    return by_layers\n",
    "\n",
    "\n",
    "def plot_boxplots():\n",
    "    single_boost_all = pd.read_csv(\"results/copyVSfact/random_attention_modif/single_boost_all.csv\")\n",
    "    single_boost_all['heads'] = single_boost_all['heads'].apply(lambda x: eval(x))\n",
    "    single_boost_all['fact_recall'] = single_boost_all['mem_win'] / 100\n",
    "    \n",
    "    single_boost_attribute = pd.read_csv(\"results/copyVSfact/random_attention_modif/single_boost_attribute.csv\")\n",
    "    single_boost_attribute['heads'] = single_boost_attribute['heads'].apply(lambda x: eval(x))\n",
    "    single_boost_attribute['fact_recall'] = single_boost_attribute['mem_win'] / 100\n",
    "\n",
    "    by_layers_all = [single_boost_all[single_boost_all['heads'].apply(lambda x: x[0] == i)] for i in range(12)]\n",
    "    by_layers_attribute = [single_boost_attribute[single_boost_attribute['heads'].apply(lambda x: x[0] == i)] for i in range(12)]\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(6, 3.5), sharey=True)\n",
    "    # ax[0].set_ylim(2, 9)\n",
    "\n",
    "    ax[0].boxplot([df['fact_recall'] for df in by_layers_all], showfliers=False)\n",
    "\n",
    "    ax[0].axhline(4.13, color='red', linestyle='--', alpha=0.3)\n",
    "    ax[0].set_title('All positions altered')\n",
    "\n",
    "    ax[1].boxplot([df['fact_recall'] for df in by_layers_attribute], showfliers=False)\n",
    "    ax[1].axhline(4.13, color='red', linestyle='--', alpha=0.3)\n",
    "    ax[1].set_title('Attribute position altered')\n",
    "\n",
    "    fig.supylabel('Factual Recall %')\n",
    "    fig.suptitle('Factual Recall by Layer')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig('layer_recall.pdf')\n",
    "\n",
    "def double_random_recall_analyse(pos):\n",
    "    double_boost = pd.read_csv(f\"results/copyVSfact/random_attention_modif/double_boost_{pos}.csv\")\n",
    "    double_boost['heads'] = double_boost['heads'].apply(lambda x: eval(x))\n",
    "    double_boost['fact_recall'] = double_boost['mem_win'] / 100\n",
    "\n",
    "    same_layer = double_boost[double_boost['heads'].apply(lambda x: x[0][0] == x[1][0])]\n",
    "    one_layer_apart = double_boost[double_boost['heads'].apply(lambda x: abs(x[0][0] - x[1][0]) == 1)]\n",
    "    med_far_away = double_boost[double_boost['heads'].apply(lambda x: 1 < abs(x[0][0] - x[1][0]) < 5)]\n",
    "    far_away = double_boost[double_boost['heads'].apply(lambda x: abs(x[0][0] - x[1][0]) > 5)]\n",
    "\n",
    "    double_stats = pd.DataFrame(([same_layer.describe()['fact_recall'], one_layer_apart.describe()['fact_recall'], med_far_away.describe()['fact_recall'], far_away.describe()['fact_recall']]))\n",
    "    double_stats.index = np.array(['same_layer', 'one_layer_apart', 'med_far_away', 'far_away'])\n",
    "    return double_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(single_random_recall_by_layer('all'))\n",
    "display(single_random_recall_by_layer('attribute'))\n",
    "display(double_random_recall_analyse('all'))\n",
    "display(double_random_recall_analyse('attribute'))\n",
    "plot_boxplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.perplexity import run as run_perplexity\n",
    "\n",
    "if os.getcwd().split(os.sep)[-1] != \"notebooks\":\n",
    "  os.chdir(\"notebooks\")\n",
    "\n",
    "if CREATE_CSVS:\n",
    "  df = run_perplexity(model=\"gpt2\")\n",
    "  df.to_csv(\"../results/modification_perplexity.csv\", index=False)\n",
    "else:\n",
    "  df = pd.read_csv(\"../results/modification_perplexity.csv\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "os.chdir(\"..\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
